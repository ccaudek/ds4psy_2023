{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-prediction)=\n",
    "# La predizione bayesiana \n",
    "\n",
    "Oltre ad una sintesi della distribuzione a posteriori attraverso il computo di indici caratteristici e alla verifica di ipotesi, un altro compito dell'analisi bayesiana è la predizione di nuovi dati futuri. Dopo aver osservato i dati di un campione e dopo avere ricavato le distribuzioni a posteriori dei parametri, è infatti possibile ottenere delle indicazioni sulle proprietà di dati futuri. L'uso più immediato della stima della distribuzione dei possibili valori futuri della variabile di esito è la verifica del modello in esame. Infatti, il modo più diretto per testare un modello è quello di utilizzare il modello corrente per fare previsioni sui possibili dati futuri per poi confrontare i dati predetti con i dati che sono stati effettivamente osservati nel campione corrente. Questa pratica va sotto il nome di *controllo predittivo a posteriori*. In questo capitolo ci focalizzeremo sul problema della predizione bayesiana esaminando il caso più semplice, ovvero lo schema beta-binomiale. In seguito estenderemo questa discussione al caso generale.\n",
    "\n",
    "## La distribuzione predittiva a posteriori\n",
    "\n",
    "Una volta costruita la distribuzione a posteriori del parametro o dei parametri sconosciuti, potremmo essere interessati a utilizzare il modello bayesiano allo scopo di prevedere la probabilità di risultati futuri basandoci sui dati già osservati. Questo tipo di analisi inferenziale va sotto il nome di *analisi predittiva*.\n",
    "\n",
    "L'esempio che considereremo qui nei dettagli riguarda il caso beta-binomiale, nel quale la distribuzione a priori per il parametro ignoto $\\theta$ (ovvero, la probabilità di successo) è una distribuzione Beta, la verosimiglianza è binomiale e i dati sono costituiti dal numero $y$ di successi in $n$ prove Bernoulliane indipendenti. Nell'esempio che discuteremo useremo un'altra volta i dati del campione di pazienti clinici depressi di @zetschefuture2019. Supponendo di volere esaminare in futuro altri $m$ pazienti clinici, ci chiediamo: quanti di essi manifesteranno una depressione grave?\n",
    "\n",
    "Siamo dunque interessati a predire i risultati che si potrebbero osservare in nuovi campioni di $m = 30$ osservazioni. Denotiamo con $\\tilde{y}$ la manifestazione della variabile casuale $\\tilde{Y}$. In un nuovo campione di $m$ osservazioni, $\\tilde{y}$ assumerà il valore $\\tilde{y}_1$ (ad es., 12), in un altro campione assumerà il valore $\\tilde{y}_2$ (ad es., 23), e così via. Siamo interessati a descrivere la probabilità che $\\tilde{y}$ assuma i valori $0, 1, 2, \\dots, 29, 30$. Tale distribuzione (in questo caso) di massa di probabilità si chiama *distribuzione predittiva a posteriori* $p(\\tilde{Y} = \\tilde{y} \\mid Y = y)$ e corrisponde alla probabilità assegnata a ciascuno dei possibili valori $\\tilde{y}$ ($0, 1, 2, \\dots, 29, 30$) nei possibili campioni futuri di $m$ osservazioni.\n",
    "\n",
    "In questo Capitolo ci porremo il problema di trovare la distribuzione predittiva a posteriori nel caso beta-binomiale. Considereremo tre metodi diversi:\n",
    "\n",
    "-   la soluzione analitica,\n",
    "-   i risultati di una simulazione,\n",
    "-   il campionamento MCMC.\n",
    "\n",
    "I tre metodi producono risultati equivalenti. In seguito useremo il metodo MCMC perché ci consente di trovare facilmente la risposta cercata, anche quando una soluzione analitica non è disponibile.\n",
    "\n",
    "## Soluzione analitica\n",
    "\n",
    "Nel caso dell'esempio in discussione, la distribuzione di $\\tilde{Y}$ dipende da $\\theta$ e ciò che sappiamo di $\\theta$ è sintetizzato nella distribuzione a posteriori $p(\\theta \\mid y)$. Usando la regola della catena, possiamo scrivere la distribuzione congiunta di $\\tilde{y}$ e $\\theta$ nel modo seguente\n",
    "\n",
    "$$\n",
    "p(\\tilde{y}, \\theta \\mid y) = p(\\tilde{y} \\mid \\theta, y) p(\\theta \\mid y).\n",
    "$$\n",
    "\n",
    "Assumendo che le osservazioni future $\\tilde{y}$ e passate $y$ siano condizionalmente indipendenti dato $\\theta$, l'espressione precedente può essere scritta come\n",
    "\n",
    "$$\n",
    "p(\\tilde{y}, \\theta \\mid y) = p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y).\n",
    "$$\n",
    "\n",
    "La distribuzione predittiva a posteriori viene ottenuta dalla distribuzione congiunta di $\\tilde{y}$ e $\\theta$ integrando rispetto a $\\theta$:\n",
    "\n",
    "$$\n",
    "p(\\tilde{y} \\mid y) = \\int_{\\theta} p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y) \\,\\operatorname {d}\\!\\theta.\n",
    "$$ (eq-post-pred-distr)\n",
    "\n",
    "Nel caso dello schema beta-binomiale, la funzione $p(\\tilde{y} \\mid \\theta)$ è binomiale di parametri $m$ e $\\theta$, e la distribuzione a posteriori $p(\\theta \\mid y)$ è una $Beta(\\alpha + y, \\beta + n - y)$. Risolvendo l'integrale otteniamo:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\tilde{y} \\mid y) &= \\int_0^1 p(\\tilde{y} \\mid \\theta)\n",
    "p(\\theta \\mid y)\\,\\operatorname {d}\\!\\theta \\notag\\\\\n",
    " &= \\int_0^1 \\begin{pmatrix}m\\\\\\tilde{y}\\end{pmatrix}\n",
    " \\theta^{\\tilde{y}}\n",
    "(1-\\theta)^{m-\\tilde{y}} \\, Beta(a+y,b+n-y) \\, d\\theta \\notag\\\\\n",
    "&= \\begin{pmatrix}{m}\\\\\\tilde{y}\\end{pmatrix} \\int_0^1 \\theta^{\\tilde{y}}\n",
    "(1-\\theta)^{m-\\tilde{y}} \\frac{1}{B(a+y, b+n-y)}\\theta^{a+y-1}(1-\\theta)^{b+n-y-1}\\notag\\\\\n",
    "&= \\begin{pmatrix}{ m }\\\\\\tilde{y}\\end{pmatrix} \\frac{1}{B(a+y, b+n-y)}\\int_0^1 \\theta^{\\tilde{y}+a+y-1}(1-\\theta)^{m-\\tilde{y}+b+n-y-1}\\notag\\\\\n",
    "&= \\begin{pmatrix}{ m }\\\\\\tilde{y}\\end{pmatrix} \\frac{B(\\tilde{y}+a+y,b+n-y+m-\\tilde{y})}{B(a+y, b+n-y)} \\; .\n",
    "\\end{align}\n",
    "$$ (eq-post-yprime-an-sol-betabin)\n",
    "\n",
    "In conclusione, per lo schema beta-binomiale, la distribuzione predittiva a posteriori corrisponde ad una distribuzione di probabilità discreta chiamata *distribuzione beta-binomiale* di parametri $m$, $\\alpha+y$ e $\\beta+n-y$.\n",
    "\n",
    "$$\n",
    "f(\\tilde{y} \\mid y) = \\binom{m}{\\tilde{y}} \\frac{B(a+ y + \\tilde{y}, b + n - y + m - \\tilde{y})}{B(a+y, b+n-y)},\n",
    "$$ (eq-beta-binomial-distr)\n",
    "\n",
    "Nell'esempio relativo allo studio di @zetschefuture2019, la verosimiglianza è binomiale, i dati sono costituiti da 23 successi su 30 prove e la distribuzione a priori su $\\theta$ è $(Beta)(2, 10)$. Di conseguenza, la distribuzione a posteriori è $(Beta)(25, 17)$. Vogliamo calcolare la distribuzione predittiva a posteriori per un nuovo campione, poniamo, di $m = 30$ osservazioni (ma, in generale, $m$ può essere diverso da $n$).\n",
    "\n",
    "In base all'eq. {eq}`eq-beta-binomial-distr` sappiamo che la distribuzione predittiva a posteriori è una distribuzione beta-binomiale di parametri $m$, $\\alpha+y$ e $\\beta+n-y$, dove $m$ è il numero di prove nel nuovo campione, $\\alpha$ e $\\beta$ sono i parametri della distribuzione a priori, e $y$ e $n$ sono le proprietà del campione corrente. Nel caso dell'esempio in discussione, $m = 30$, $\\alpha = 2 + 23 = 25$, $\\beta = 10 + 30 - 23 = 17$. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Jun  2 2021, 07:05:41) \n[Clang 12.0.5 (clang-1205.0.22.9)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c300c63927c1d487b2df0ca7f55d9699efd42c18a65d5b05a5383e492fa5764c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
